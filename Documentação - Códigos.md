
# üßæ Documenta√ß√£o T√©cnica - Web Scraping Mercado Livre

Este script realiza a **extra√ß√£o automatizada de dados de produtos para c√£es** no site Mercado Livre, utilizando `Selenium`, `BeautifulSoup` e `Requests`.

---

## üì¶ Requisitos

Instale os pacotes necess√°rios:

```bash
pip install requests
pip install beautifulsoup4
pip install selenium
```

---

## üîó URL de origem

```python
url = "https://lista.mercadolivre.com.br/_Container_pet-cpg-caes"
```

---

## üöÄ 1. Inicializa√ß√£o do Selenium

```python
from selenium import webdriver
from selenium.webdriver.edge.service import Service

edge_service = Service(r"Caminho\para\msedgedriver.exe")
options = webdriver.EdgeOptions()
driver = webdriver.Edge(service=edge_service, options=options)
driver.get(url)
```

- Inicia o navegador Edge automatizado
- Acessa a URL de listagem de produtos

---

## üîÑ 2. Extra√ß√£o de links de produtos

```python
produtos_links = []

title_wrappers = driver.find_elements(By.CLASS_NAME,"poly-component__title-wrapper")
for wrapper in title_wrappers:
    link_element = wrapper.find_element(By.TAG_NAME, "a")
    produtos_links.append(link_element.get_attribute("href"))
```

- Captura os links dos produtos em cada p√°gina
- Navega para a pr√≥xima p√°gina via bot√£o "Pr√≥ximo"
- Repeti√ß√£o at√© n√£o haver mais p√°ginas

---

## üíæ 3. Armazenamento dos links

```python
df = pd.DataFrame(produtos_links)
df.to_csv('URLs_por_Produto.csv', index=False)
```

- Links s√£o salvos em `URLs_por_Produto.csv`

---

## üßÆ 4. Estrutura dos Datasets

S√£o utilizados **4 dicion√°rios principais**:

```python
dataset1 = {'Categoria': [], 'Produto': [], 'Marca': [], 'Preco': [], 'Avaliacao': [], 'Quant. Avaliacoes': [], 'Quant. Coment√°rios': [], 'URL': []}
dataset2 = {'URL': [], 'Coment√°rios': []}
dataset3 = {'Caracter√≠stica': [], 'Aval.Caracter√≠stica': [], 'URL': []}
nao_capturado = []
```

---

## üîç 5. Loop de extra√ß√£o por URL

```python
for pag in product_links:
    try:
        req_books = requests.get(pag)
        conteudo_books = BeautifulSoup(req_books.text, 'html.parser')
        ...
    except Exception as e:
        nao_capturado.append(pag)
```

Cada URL passa pelas seguintes etapas:

### üõí Informa√ß√µes gerais

- `produto`: t√≠tulo da p√°gina
- `preco`: valor extra√≠do de `<meta itemprop="price">`
- `categoria`: breadcrumb final
- `avaliacao`: m√©dia de notas
- `quant_avaliacao`: quantidade de avalia√ß√µes
- `quant_comentario`: quantidade de coment√°rios (se existir)
- `marca`: varre as tabelas para encontrar linha com t√≠tulo "Marca"

### üó£Ô∏è Coment√°rios

Dois fluxos poss√≠veis:

1. **Com modal (iframe)**: usa Selenium para abrir coment√°rios em p√°gina nova  
2. **Sem modal**: extrai diretamente com BeautifulSoup

### üìä Caracter√≠sticas avaliadas

- Encontra tabelas com classe `ui-review-capability-categories__desktop--row`
- Extrai: `Caracter√≠stica`, `Avalia√ß√£o`, e URL

---

## üìâ 6. Controle de Requisi√ß√µes

```python
requisicoes += 1
if requisicoes == 100:
    time.sleep(1805)
```

Evita bloqueio do servidor fazendo pausa de 30 minutos a cada 100 requisi√ß√µes.

---

## üì§ 7. Exporta√ß√£o dos resultados

```python
pd.DataFrame(dataset1).to_csv('df1.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(dataset2).to_csv('df2.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(dataset3).to_csv('df3.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(nao_capturado).to_csv('urls_faltantes.csv', sep=';')
```

---

## üß† Observa√ß√µes T√©cnicas

- A captura de algumas informa√ß√µes podem falhar devido a falha ou demora de carregamento do html. Isso √© tratado com tentativas repetidas.
- A rolagem infinita √© simulada via `PAGE_DOWN` e checagem de `scrollTop`.
- Bot√£o ‚ÄúVer mais‚Äù de coment√°rios √© clicado com `Selenium` e `ActionChains`.

---

## ‚úÖ Futuras a√ß√µes

- Modularizar cada etapa (coleta de links, coleta de dados, coleta de coment√°rios)
- Centralizar controle de exce√ß√µes e log
- Criar arquivos `.py` separados para scraping e processamento
- Parametrizar caminho do driver e URL de origem
- Adicionar `try/except` com tipos de exce√ß√£o espec√≠ficas
- Otimizar checagem de ‚ÄúMarca‚Äù com regex

---

## üß™ Testes e Valida√ß√£o

- Testado com dezenas de produtos da categoria PET
- URLs com falhas s√£o salvas em `urls_faltantes.csv`
- C√≥digo resiste a pequenas varia√ß√µes de layout

---

# üìÑ Documenta√ß√£o  Gera√ß√£o de √çndice de URLs `IDs_URLs.csv`

## üéØ Objetivo

Este notebook tem como objetivo gerar um √≠ndice √∫nico para cada URL dos datasets extra√≠dos via web scraping, possibilitando uma substitui√ß√£o consistente da coluna `URL` por `ID` em m√∫ltiplos datasets (df1, df2, df3).

---

## üì¶ Instala√ß√£o de Depend√™ncia

```python
get_ipython().system('pip install pyspark')
```

Instala o pacote PySpark, caso ainda n√£o esteja presente no ambiente.

---

## üöÄ Inicializa√ß√£o da SparkSession

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ID_URL").getOrCreate()
```

Cria√ß√£o da sess√£o Spark com nome `ID_URL`.

---

## üß∞ Importa√ß√µes Auxiliares

```python
from pyspark.sql.functions import countDistinct, col, count, when, monotonically_increasing_id
```

Fun√ß√µes para contagem de valores distintos, verifica√ß√£o de nulos e gera√ß√£o de IDs √∫nicos.

---

## üì• Leitura dos Dados

```python
df = spark.read.csv("dados\df1.csv", sep=";", header=True)
df.show()
```

Leitura do dataset `df1.csv` contendo as URLs.

---

## üîç Extra√ß√£o da Coluna `URL`

```python
df_indice_urls_1 = df.select("URL")
df_indice_urls_1.show()
df_indice_urls_1.count()
df_indice_urls_1.select(countDistinct(col("URL"))).show()
```

Isolamento da coluna `URL`, contagem total e contagem de valores distintos.

---

## üßº Verifica√ß√£o de Nulos

```python
df_indice_urls_1.select(count(when(col("URL").isNull(), True))).show()
```

Verifica se h√° valores nulos na coluna `URL`.

---

## üì• Leitura dos Datasets Adicionais

```python
df2 = spark.read.csv("dados\df2.csv", sep=";", header=True)
df3 = spark.read.csv("dados\df3.csv", sep=";", header=True)
```

Leitura dos arquivos `df2.csv` e `df3.csv` para comparar URLs.

---

## üîç Coleta das URLs dos Datasets df2 e df3

```python
df_indice_urls_2 = df.select("URL")
df_indice_urls_3 = df.select("URL")
```

Extrai-se a coluna `URL` para an√°lise.

---

## üìä An√°lise de URLs Duplicadas e Nulas

```python
df_indice_urls_2.count()
df_indice_urls_2.select(countDistinct(col("URL"))).show()
df_indice_urls_2.select(count(when(col("URL").isNull(), True))).show()

df_indice_urls_3.count()
df_indice_urls_3.select(countDistinct(col("URL"))).show()
df_indice_urls_3.select(count(when(col("URL").isNull(), True))).show()
```

Valida√ß√µes semelhantes √†s realizadas para `df1`, aplicadas a `df2` e `df3`.

---

## üßæ Convers√£o de URLs para Lista

```python
lista1 = [row["URL"] for row in urls_1.select("URL").collect()]
lista2 = [row["URL"] for row in urls_2.select("URL").collect()]
lista3 = [row["URL"] for row in urls_3.select("URL").collect()]
```

Converte as URLs distintas de cada dataset para listas Python.

---

## üîé Verifica√ß√£o de URLs N√£o Presentes

```python
urls = [i for i in lista1 if i not in lista2]
urls = [i for i in lista1 if i not in lista3]
```

Identifica URLs presentes em `df1`, mas ausentes em `df2` ou `df3`.

---

## üÜî Gera√ß√£o de Identificadores √önicos

```python
id_url = urls_1.withColumn("ID", monotonically_increasing_id())
id_url.write.csv("dados\IDs_URLs.csv", sep=";", header=True)
```

Cria uma nova coluna `ID` com identificadores √∫nicos para cada URL distinta e salva o resultado no arquivo `IDs_URLs.csv`.

---

## ‚úÖ Resultado Final

O arquivo `IDs_URLs.csv` cont√©m todas as URLs √∫nicas de `df1` com um identificador exclusivo. Esse arquivo √© utilizado para substituir a coluna `URL` por `ID` nos demais datasets.


# Documenta√ß√£o do Pr√©-processamento do Dataset `df1`

Este documento descreve o tratamento aplicado ao dataset `df1`, desenvolvido em Apache Spark com PySpark. O objetivo foi limpar, padronizar e preparar os dados para an√°lises posteriores.

## Sum√°rio

- [1. Carregamento dos Dados](#1-carregamento-dos-dados)
- [2. Padroniza√ß√£o de Vari√°veis e Valores](#2-padroniza√ß√£o-de-vari√°veis-e-valores)
- [3. Tratamento de Campos Nulos](#3-tratamento-de-campos-nulos)
- [4. Remo√ß√£o de Registros Duplicados](#4-remo√ß√£o-de-registros-duplicados)
- [5. Tratamento da Vari√°vel `Categoria`](#5-tratamento-da-vari√°vel-categoria)
- [6. Tratamento da Vari√°vel `Marca`](#6-tratamento-da-vari√°vel-marca)
- [7. Substitui√ß√£o da Coluna `URL` por `ID`](#7-substitui√ß√£o-da-coluna-url-por-id)
- [8. Exporta√ß√£o do Dataset](#8-exporta√ß√£o-do-dataset)

---

## 1. Carregamento dos Dados

- Inicializa√ß√£o da SparkSession.
- Leitura do dataset `df1.csv` com `;` como delimitador e cabe√ßalho ativado.

## 2. Padroniza√ß√£o de Vari√°veis e Valores

- Renomeadas colunas para evitar caracteres especiais.
- Substitu√≠dos caracteres n√£o num√©ricos em `Quant_Comentarios`.
- Tipos de dados convertidos para `FloatType` e `IntegerType` quando aplic√°vel.

## 3. Tratamento de Campos Nulos

- Identifica√ß√£o de valores nulos por coluna.
- Substitui√ß√£o de nulos nas colunas `Quant_Avaliacoes` e `Quant_Comentarios` por 0.
- Substitui√ß√£o de nulos na coluna `Marca` por `'Nao Informado'`.

## 4. Remo√ß√£o de Registros Duplicados

- Elimina√ß√£o de registros duplicados com `dropDuplicates()`.

## 5. Tratamento da Vari√°vel `Categoria`

- Normaliza√ß√£o de valores de `Categoria` com base em palavras-chave presentes nas strings.
- Unifica√ß√£o de categorias similares para agrupamentos mais consistentes.
- Considera√ß√£o tamb√©m de valores presentes na coluna `Produto`.

## 6. Tratamento da Vari√°vel `Marca`

- Corre√ß√µes e padroniza√ß√µes nos valores da coluna `Marca`.
- Exclus√£o de termos gen√©ricos ou inv√°lidos substituindo-os por `'Nao Informado'`.

## 7. Substitui√ß√£o da Coluna `URL` por `ID`

- Leitura do dataset `IDs_URLs.csv`.
- Jun√ß√£o com o dataset tratado com base na coluna `URL`.
- Remo√ß√£o da coluna `URL`.

## 8. Exporta√ß√£o do Dataset

- Escrita final dos dados tratados no formato `.parquet` com o nome `df1_tratado.parquet`.

---

## Considera√ß√µes Finais

O dataset `df1` passou por um processo completo de ETL (Extract, Transform, Load), garantindo:

- Dados limpos e estruturados;
- Tipos de dados coerentes para an√°lise;
- Categoriza√ß√£o uniforme e padronizada;
- Base pronta para integra√ß√£o e uso em pipelines de an√°lise de dados ou machine learning.

# Documenta√ß√£o de Tratamento de Dados ‚Äî Dataset `df2`

## üìå Objetivo
Este notebook realiza o tratamento do dataset `df2.csv`, contendo informa√ß√µes de coment√°rios de produtos. O tratamento tem como finalidade a limpeza dos dados e a substitui√ß√£o da vari√°vel `URL` pela vari√°vel `ID`, utilizando uma tabela auxiliar.

---

## ‚öôÔ∏è Etapas do Processamento

### 1. Cria√ß√£o da SparkSession
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Prata3").getOrCreate()
```
Inicializa√ß√£o do ambiente Spark para manipula√ß√£o dos dados.

---

### 2. Importa√ß√µes
```python
from pyspark.sql.functions import col, sum, count, when, countDistinct
```
Fun√ß√µes auxiliares para opera√ß√µes de filtragem, contagem e renomea√ß√£o de colunas.

---

### 3. Leitura dos Dados
```python
df = spark.read.csv("dados\df2.csv", sep=";", header=True)
```
Leitura do dataset `df2.csv` com separador `;` e cabe√ßalho.

---

### 4. Visualiza√ß√£o e Contagem Inicial
```python
df.show()
df.count()
```
Visualiza√ß√£o inicial dos dados e contagem do n√∫mero total de registros.

---

### 5. Renomea√ß√£o de Coluna
```python
df = df.withColumnRenamed("Coment√°rios", "Comentarios")
```
Padroniza√ß√£o do nome da coluna `Coment√°rios` para `Comentarios`.

---

### 6. Verifica√ß√£o de Valores Nulos
```python
df.filter(col("Comentarios").isNull()).count()
df.select(count(when(df["Comentarios"].isNull(), True)).alias("Quant_Nulos")).show()
```
Contagem de registros com valores nulos na coluna `Comentarios`.

---

### 7. Remo√ß√£o de Registros com Nulos
```python
df_sem_nulos = df.dropna()
df_sem_nulos.count()
df_sem_nulos.select(count(when(col("Comentarios").isNull(),True)).alias("Quant_Nulos")).show()
```
Remo√ß√£o de registros com valores nulos e verifica√ß√£o p√≥s-tratamento.

---

### 8. Verifica√ß√£o de Unicidade da Coluna `URL`
```python
df.select(countDistinct(col("URL"))).show()
df_sem_nulos.select(countDistinct(col("URL"))).show()
```
Verifica√ß√£o da quantidade de URLs distintas antes e depois do tratamento.

---

### 9. Substitui√ß√£o da Coluna `URL` por `ID`
```python
id_urls = spark.read.csv("dados\IDs_URLs.csv", sep=";", header=True)
df2_tratado = id_urls.join(df_sem_nulos, on="URL", how="inner").drop("URL")
```
Utiliza√ß√£o do dataset `IDs_URLs.csv` para substituir a coluna `URL` pela `ID` correspondente.

---

### 10. Escrita do Dataset Final
```python
df2_tratado.write.parquet("dados\df2_tratado.parquet")
```
Salvamento do dataset tratado em formato `.parquet`.

---

### 11. Verifica√ß√£o Final do Schema
```python
df2_tratado.printSchema()
```
Visualiza√ß√£o do schema final para confer√™ncia dos tipos e nomes das colunas.

---

## ‚úÖ Resultado
O dataset `df2` foi tratado com sucesso, resultando em um arquivo `df2_tratado.parquet` sem valores nulos e com a substitui√ß√£o de `URL` por `ID`.


# Documenta√ß√£o de Tratamento de Dados ‚Äî Dataset `df3`

## üéØ Objetivo

Realizar o tratamento dos dados contidos no arquivo `df3.csv`, padronizando valores, removendo inconsist√™ncias e preparando o dataset para integra√ß√µes e an√°lises posteriores.

---

## üîß Importa√ß√µes

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import count_distinct, col, when, sum, regexp_extract
```

---

## üöÄ Inicializa√ß√£o da SparkSession

```python
spark = SparkSession.builder.appName('Prata2').getOrCreate()
```

Cria√ß√£o da sess√£o Spark com o nome `Prata2`.

---

## üì• Leitura dos Dados

```python
df = spark.read.csv("dados\\df3.csv", sep=";", header=True)
df.show()
df.count()
df.printSchema()
```

Leitura do arquivo CSV contendo as avalia√ß√µes de caracter√≠sticas dos produtos. Exibe os dados, conta as linhas e imprime o schema.

---

## ‚úçÔ∏è Renomea√ß√£o de Colunas

```python
df = df.withColumnRenamed("Aval.Caracter√≠stica", "Aval_Caracteristica") \       .withColumnRenamed("Caracter√≠stica", "Caracteristica")
```

Padroniza√ß√£o dos nomes de colunas para evitar problemas com caracteres especiais.

---

## üîç An√°lise de Distintos

```python
df.select("Caracteristica").distinct().withColumnRenamed("Caracteristica", "Caracteristiscas_Unicas").show(truncate=False)
df.select(count_distinct(col("Caracteristica")).alias("Caracteristicas_Distintas")).show()
```

Verifica a diversidade de caracter√≠sticas no dataset.

---

## üîÅ Padroniza√ß√£o de Valores Categ√≥ricos

```python
df1 = df.withColumn("Caracteristica", when(col("Caracteristica") == "Relaci√≥n precio-calidad", "Rela√ß√£o pre√ßo-qualidade")
                    .otherwise(col("Caracteristica"))) \        .withColumn("Caracteristica", when(col("Caracteristica") == "Calidad de los materiais", "Qualidade dos materiais")
                    .otherwise(col("Caracteristica")))
```

Padroniza valores escritos em espanhol para o portugu√™s.

---

## ‚ùì Verifica√ß√£o de Valores Nulos

```python
nulos = df.select([
    sum(col("Aval_Caracteristica").isNull().cast("int")).alias("Quant_Nulos"),
    sum(col("Caracteristica").isNull().cast("int")).alias("Quant_Caracteristica"),
    sum(col("URL").isNull().cast("int")).alias("Quant_URL")
])
nulos.show()
```

Conta os valores nulos em cada coluna.

---

## üî¢ Extra√ß√£o de N√∫meros da Coluna de Avalia√ß√£o

```python
df2 = df.withColumn("Aval_Caracteristica", regexp_extract("Aval_Caracteristica", r"(\d+(\.\d+)?)", 1))
df2.show()
```

Extrai os valores num√©ricos das strings na coluna `Aval_Caracteristica`.

---

## üîÑ Convers√£o de Tipos

```python
df3 = df2.withColumn("Aval_Caracteristica", col("Aval_Caracteristica").cast("double"))
df3.printSchema()
df3.show()
df3.count()
```

Converte a coluna de avalia√ß√£o para tipo `double`.

---

## üîó Jun√ß√£o com IDs de URLs

```python
id_urls = spark.read.csv("dados\\IDs_URLs.csv", sep=";", header=True)
df3_tratado = id_urls.join(df3, on="URL", how="inner").drop("URL")
df3_tratado.show()
```

Relaciona os dados tratados com um identificador √∫nico por URL, removendo a URL do dataset final.

---

## üíæ Escrita do Dataset Tratado

```python
df3_tratado.write.parquet("dados\\df3_tratado.parquet")
df3_tratado.printSchema()
```

Escreve os dados tratados no formato Parquet para uso futuro.

---

## ‚úÖ Considera√ß√µes Finais

O dataset `df3` foi padronizado, limpo e enriquecido com identificadores √∫nicos. Isso permite sua integra√ß√£o com outras tabelas e an√°lise em modelos como o modelo estrela.

# üìÑ Documenta√ß√£o Constru√ß√£o do Modelo Estrela

## üéØ Objetivo

Este notebook realiza a transforma√ß√£o e modelagem dos dados coletados via web scraping, estruturando-os em um **modelo estrela** para an√°lises de BI. As etapas envolvem o c√°lculo de m√©tricas derivadas, categoriza√ß√£o de pre√ßos, identifica√ß√£o de produtos destaque e a constru√ß√£o de tabelas fato e dimens√µes.

---

## üì¶ Importa√ß√µes

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum, when, countDistinct, min, max, avg, round, lit, row_number, desc, asc, log1p, percent_rank, upper
from pyspark.sql.window import Window
```

Fun√ß√µes utilizadas para agrega√ß√µes, transforma√ß√µes e janelas de an√°lise.

---

## üöÄ Inicializa√ß√£o da SparkSession

```python
spark = SparkSession.builder.appName('tabelas').getOrCreate()
```

Cria o ambiente Spark necess√°rio para manipula√ß√£o dos dados.

---

## üì• Carregamento dos Datasets Tratados

```python
df1 = spark.read.parquet(".../df1_tratado.parquet")
df3 = spark.read.parquet(".../df3_tratado.parquet")
df4 = spark.read.csv(".../IDs_URLs.csv", sep=';', header=True)
```

Leitura dos datasets finais j√° tratados e prontos para modelagem.

---

## üìä Avalia√ß√£o Ponderada

```python
media_avaliacoes_produtos = df1.select(avg(col("Avaliacao"))).collect()[0][0]
m = 10  # valor m√≠nimo de confian√ßa
df_transicao = df1.withColumn(
    'avaliacao_ponderada',
    (col('Quant_Avaliacoes') / (col('Quant_Avaliacoes') + lit(m))) * col('Avaliacao') +
    ((lit(m) / (col('Quant_Avaliacoes') + lit(m))) * lit(media_avaliacoes_produtos)))
```

Calcula uma avalia√ß√£o mais justa ponderando pela quantidade de avalia√ß√µes.

---

## üìà Escore de Engajamento

```python
df_transicao = df_transicao.withColumn(
    'Escore_Engajamento',
    log1p(col('Quant_Avaliacoes') + col('Quant_Comentarios')))
```

Cria um escore baseado na soma de avalia√ß√µes e coment√°rios com escala logar√≠tmica.

---

## üí∞ Escore Custo-Benef√≠cio

```python
df_transicao = df_transicao.withColumn(
    'Escore_Custo_Beneficio',
    when(col('Preco') > 0, col('avaliacao_ponderada') / col('Preco')).otherwise(lit(None)))
```

Mede a rela√ß√£o custo-benef√≠cio do produto.

---

## üè∑Ô∏è Faixa de Pre√ßo

```python
quantis = df_transicao.approxQuantile("Preco", [0.33, 0.66], 0.01)
df_transicao = df_transicao.withColumn(
    "Faixa_Preco",
    when(col("Preco") <= quantis[0], "Baixo")
    .when((col("Preco") > quantis[0]) & (col("Preco") <= quantis[1]), "M√©dio")
    .otherwise("Alto"))
```

Agrupamento de produtos por faixa de pre√ßo baseada em quantis.

---

## üåü Produto Destaque

```python
mediana_engajamento = df_transicao.approxQuantile("Escore_Engajamento", [0.5], 0.01)[0]
df_transicao = df_transicao.withColumn(
    "Produto_Destaque",
    when((col("avaliacao_ponderada") >= 4.5) & (col("Escore_Engajamento") >= mediana_engajamento), True).otherwise(False))
```

Marca produtos com boa avalia√ß√£o e alto engajamento como destaque.

---

## üèÅ Cria√ß√£o da Tabela Fato

```python
fato_avaliacoes_produto = df_transicao
```

Tabela principal que consolida todas as m√©tricas calculadas.

---

## üì¶ Tabelas de Dimens√£o

### Produto

```python
dim_produto = df1.select('ID', 'Produto', 'Categoria', 'Marca')
```

### Caracter√≠sticas

```python
dim_caracteristicas = df3
```

### URLs

```python
dim_url = df4
```

### Marca

```python
dim_marca = fato_avaliacoes_produto.groupBy("Marca").agg(...).withColumn("Faixa_Preco_Marca", ...)
```

### Categoria

```python
dim_categoria = fato_avaliacoes_produto.groupBy("Categoria").agg(...).withColumn("Faixa_Preco_Categoria", ...)
```

Cada dimens√£o agrega dados por chave descritiva, com m√©tricas e classifica√ß√µes por faixa.

---

## üíæ Escrita dos Dados

```python
fato_avaliacoes_produto.write.parquet(".../fato_avaliacoes_produto")
dim_produto.write.parquet(".../dim_produto")
dim_caracteristicas.write.parquet(".../dim_caracteristicas")
dim_url.write.parquet(".../dim_url")
dim_marca.write.parquet(".../dim_marca")
dim_categoria.write.parquet(".../dim_categoria")
```

Armazena todas as tabelas geradas no formato `.parquet`.

---

## ‚úÖ Resultado Final

Estrutura final em modelo estrela, pronta para an√°lise em ferramentas de BI ou explora√ß√£o com SparkSQL.
