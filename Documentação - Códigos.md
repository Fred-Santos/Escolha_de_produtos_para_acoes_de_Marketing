
# ğŸ§¾ DocumentaÃ§Ã£o TÃ©cnica - Web Scraping Mercado Livre

Este script realiza a **extraÃ§Ã£o automatizada de dados de produtos para cÃ£es** no site Mercado Livre, utilizando `Selenium`, `BeautifulSoup` e `Requests`.

---

## ğŸ“¦ Requisitos

Instale os pacotes necessÃ¡rios:

```bash
pip install requests
pip install beautifulsoup4
pip install selenium
```

---

## ğŸ”— URL de origem

```python
url = "https://lista.mercadolivre.com.br/_Container_pet-cpg-caes"
```

---

## ğŸš€ 1. InicializaÃ§Ã£o do Selenium

```python
from selenium import webdriver
from selenium.webdriver.edge.service import Service

edge_service = Service(r"Caminho\para\msedgedriver.exe")
options = webdriver.EdgeOptions()
driver = webdriver.Edge(service=edge_service, options=options)
driver.get(url)
```

- Inicia o navegador Edge automatizado
- Acessa a URL de listagem de produtos

---

## ğŸ”„ 2. ExtraÃ§Ã£o de links de produtos

```python
produtos_links = []

title_wrappers = driver.find_elements(By.CLASS_NAME,"poly-component__title-wrapper")
for wrapper in title_wrappers:
    link_element = wrapper.find_element(By.TAG_NAME, "a")
    produtos_links.append(link_element.get_attribute("href"))
```

- Captura os links dos produtos em cada pÃ¡gina
- Navega para a prÃ³xima pÃ¡gina via botÃ£o "PrÃ³ximo"
- RepetiÃ§Ã£o atÃ© nÃ£o haver mais pÃ¡ginas

---

## ğŸ’¾ 3. Armazenamento dos links

```python
df = pd.DataFrame(produtos_links)
df.to_csv('URLs_por_Produto.csv', index=False)
```

- Links sÃ£o salvos em `URLs_por_Produto.csv`

---

## ğŸ§® 4. Estrutura dos Datasets

SÃ£o utilizados **4 dicionÃ¡rios principais**:

```python
dataset1 = {'Categoria': [], 'Produto': [], 'Marca': [], 'Preco': [], 'Avaliacao': [], 'Quant. Avaliacoes': [], 'Quant. ComentÃ¡rios': [], 'URL': []}
dataset2 = {'URL': [], 'ComentÃ¡rios': []}
dataset3 = {'CaracterÃ­stica': [], 'Aval.CaracterÃ­stica': [], 'URL': []}
nao_capturado = []
```

---

## ğŸ” 5. Loop de extraÃ§Ã£o por URL

```python
for pag in product_links:
    try:
        req_books = requests.get(pag)
        conteudo_books = BeautifulSoup(req_books.text, 'html.parser')
        ...
    except Exception as e:
        nao_capturado.append(pag)
```

Cada URL passa pelas seguintes etapas:

### ğŸ›’ InformaÃ§Ãµes gerais

- `produto`: tÃ­tulo da pÃ¡gina
- `preco`: valor extraÃ­do de `<meta itemprop="price">`
- `categoria`: breadcrumb final
- `avaliacao`: mÃ©dia de notas
- `quant_avaliacao`: quantidade de avaliaÃ§Ãµes
- `quant_comentario`: quantidade de comentÃ¡rios (se existir)
- `marca`: varre as tabelas para encontrar linha com tÃ­tulo "Marca"

### ğŸ—£ï¸ ComentÃ¡rios

Dois fluxos possÃ­veis:

1. **Com modal (iframe)**: usa Selenium para abrir comentÃ¡rios em pÃ¡gina nova  
2. **Sem modal**: extrai diretamente com BeautifulSoup

### ğŸ“Š CaracterÃ­sticas avaliadas

- Encontra tabelas com classe `ui-review-capability-categories__desktop--row`
- Extrai: `CaracterÃ­stica`, `AvaliaÃ§Ã£o`, e URL

---

## ğŸ“‰ 6. Controle de RequisiÃ§Ãµes

```python
requisicoes += 1
if requisicoes == 100:
    time.sleep(1805)
```

Evita bloqueio do servidor fazendo pausa de 30 minutos a cada 100 requisiÃ§Ãµes.

---

## ğŸ“¤ 7. ExportaÃ§Ã£o dos resultados

```python
pd.DataFrame(dataset1).to_csv('df1.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(dataset2).to_csv('df2.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(dataset3).to_csv('df3.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(nao_capturado).to_csv('urls_faltantes.csv', sep=';')
```

---

## ğŸ§  ObservaÃ§Ãµes TÃ©cnicas

- A captura de algumas informaÃ§Ãµes podem falhar devido a falha ou demora de carregamento do html. Isso Ã© tratado com tentativas repetidas.
- A rolagem infinita Ã© simulada via `PAGE_DOWN` e checagem de `scrollTop`.
- BotÃ£o â€œVer maisâ€ de comentÃ¡rios Ã© clicado com `Selenium` e `ActionChains`.

---

## âœ… Futuras aÃ§Ãµes

- Modularizar cada etapa (coleta de links, coleta de dados, coleta de comentÃ¡rios)
- Centralizar controle de exceÃ§Ãµes e log
- Criar arquivos `.py` separados para scraping e processamento
- Parametrizar caminho do driver e URL de origem
- Adicionar `try/except` com tipos de exceÃ§Ã£o especÃ­ficas
- Otimizar checagem de â€œMarcaâ€ com regex

---

## ğŸ§ª Testes e ValidaÃ§Ã£o

- Testado com dezenas de produtos da categoria PET
- URLs com falhas sÃ£o salvas em `urls_faltantes.csv`
- CÃ³digo resiste a pequenas variaÃ§Ãµes de layout

---

# ğŸ“„ DocumentaÃ§Ã£o  GeraÃ§Ã£o de Ãndice de URLs `IDs_URLs.csv`

## ğŸ¯ Objetivo

Este notebook tem como objetivo gerar um Ã­ndice Ãºnico para cada URL dos datasets extraÃ­dos via web scraping, possibilitando uma substituiÃ§Ã£o consistente da coluna `URL` por `ID` em mÃºltiplos datasets (df1, df2, df3).

---

## ğŸ“¦ InstalaÃ§Ã£o de DependÃªncia

```python
get_ipython().system('pip install pyspark')
```

Instala o pacote PySpark, caso ainda nÃ£o esteja presente no ambiente.

---

## ğŸš€ InicializaÃ§Ã£o da SparkSession

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ID_URL").getOrCreate()
```

CriaÃ§Ã£o da sessÃ£o Spark com nome `ID_URL`.

---

## ğŸ§° ImportaÃ§Ãµes Auxiliares

```python
from pyspark.sql.functions import countDistinct, col, count, when, monotonically_increasing_id
```

FunÃ§Ãµes para contagem de valores distintos, verificaÃ§Ã£o de nulos e geraÃ§Ã£o de IDs Ãºnicos.

---

## ğŸ“¥ Leitura dos Dados

```python
df = spark.read.csv("dados\df1.csv", sep=";", header=True)
df.show()
```

Leitura do dataset `df1.csv` contendo as URLs.

---

## ğŸ” ExtraÃ§Ã£o da Coluna `URL`

```python
df_indice_urls_1 = df.select("URL")
df_indice_urls_1.show()
df_indice_urls_1.count()
df_indice_urls_1.select(countDistinct(col("URL"))).show()
```

Isolamento da coluna `URL`, contagem total e contagem de valores distintos.

---

## ğŸ§¼ VerificaÃ§Ã£o de Nulos

```python
df_indice_urls_1.select(count(when(col("URL").isNull(), True))).show()
```

Verifica se hÃ¡ valores nulos na coluna `URL`.

---

## ğŸ“¥ Leitura dos Datasets Adicionais

```python
df2 = spark.read.csv("dados\df2.csv", sep=";", header=True)
df3 = spark.read.csv("dados\df3.csv", sep=";", header=True)
```

Leitura dos arquivos `df2.csv` e `df3.csv` para comparar URLs.

---

## ğŸ” Coleta das URLs dos Datasets df2 e df3

```python
df_indice_urls_2 = df.select("URL")
df_indice_urls_3 = df.select("URL")
```

Extrai-se a coluna `URL` para anÃ¡lise.

---

## ğŸ“Š AnÃ¡lise de URLs Duplicadas e Nulas

```python
df_indice_urls_2.count()
df_indice_urls_2.select(countDistinct(col("URL"))).show()
df_indice_urls_2.select(count(when(col("URL").isNull(), True))).show()

df_indice_urls_3.count()
df_indice_urls_3.select(countDistinct(col("URL"))).show()
df_indice_urls_3.select(count(when(col("URL").isNull(), True))).show()
```

ValidaÃ§Ãµes semelhantes Ã s realizadas para `df1`, aplicadas a `df2` e `df3`.

---

## ğŸ§¾ ConversÃ£o de URLs para Lista

```python
lista1 = [row["URL"] for row in urls_1.select("URL").collect()]
lista2 = [row["URL"] for row in urls_2.select("URL").collect()]
lista3 = [row["URL"] for row in urls_3.select("URL").collect()]
```

Converte as URLs distintas de cada dataset para listas Python.

---

## ğŸ” VerificaÃ§Ã£o de URLs NÃ£o Presentes

```python
urls = [i for i in lista1 if i not in lista2]
urls = [i for i in lista1 if i not in lista3]
```

Identifica URLs presentes em `df1`, mas ausentes em `df2` ou `df3`.

---

## ğŸ†” GeraÃ§Ã£o de Identificadores Ãšnicos

```python
id_url = urls_1.withColumn("ID", monotonically_increasing_id())
id_url.write.csv("dados\IDs_URLs.csv", sep=";", header=True)
```

Cria uma nova coluna `ID` com identificadores Ãºnicos para cada URL distinta e salva o resultado no arquivo `IDs_URLs.csv`.

---

## âœ… Resultado Final

O arquivo `IDs_URLs.csv` contÃ©m todas as URLs Ãºnicas de `df1` com um identificador exclusivo. Esse arquivo Ã© utilizado para substituir a coluna `URL` por `ID` nos demais datasets.


# DocumentaÃ§Ã£o do PrÃ©-processamento do Dataset `df1`

Este documento descreve o tratamento aplicado ao dataset `df1`, desenvolvido em Apache Spark com PySpark. O objetivo foi limpar, padronizar e preparar os dados para anÃ¡lises posteriores.

## SumÃ¡rio

- [1. Carregamento dos Dados](#1-carregamento-dos-dados)
- [2. PadronizaÃ§Ã£o de VariÃ¡veis e Valores](#2-padronizaÃ§Ã£o-de-variÃ¡veis-e-valores)
- [3. Tratamento de Campos Nulos](#3-tratamento-de-campos-nulos)
- [4. RemoÃ§Ã£o de Registros Duplicados](#4-remoÃ§Ã£o-de-registros-duplicados)
- [5. Tratamento da VariÃ¡vel `Categoria`](#5-tratamento-da-variÃ¡vel-categoria)
- [6. Tratamento da VariÃ¡vel `Marca`](#6-tratamento-da-variÃ¡vel-marca)
- [7. SubstituiÃ§Ã£o da Coluna `URL` por `ID`](#7-substituiÃ§Ã£o-da-coluna-url-por-id)
- [8. ExportaÃ§Ã£o do Dataset](#8-exportaÃ§Ã£o-do-dataset)

---

## 1. Carregamento dos Dados

- InicializaÃ§Ã£o da SparkSession.
- Leitura do dataset `df1.csv` com `;` como delimitador e cabeÃ§alho ativado.

## 2. PadronizaÃ§Ã£o de VariÃ¡veis e Valores

- Renomeadas colunas para evitar caracteres especiais.
- SubstituÃ­dos caracteres nÃ£o numÃ©ricos em `Quant_Comentarios`.
- Tipos de dados convertidos para `FloatType` e `IntegerType` quando aplicÃ¡vel.

## 3. Tratamento de Campos Nulos

- IdentificaÃ§Ã£o de valores nulos por coluna.
- SubstituiÃ§Ã£o de nulos nas colunas `Quant_Avaliacoes` e `Quant_Comentarios` por 0.
- SubstituiÃ§Ã£o de nulos na coluna `Marca` por `'Nao Informado'`.

## 4. RemoÃ§Ã£o de Registros Duplicados

- EliminaÃ§Ã£o de registros duplicados com `dropDuplicates()`.

## 5. Tratamento da VariÃ¡vel `Categoria`

- NormalizaÃ§Ã£o de valores de `Categoria` com base em palavras-chave presentes nas strings.
- UnificaÃ§Ã£o de categorias similares para agrupamentos mais consistentes.
- ConsideraÃ§Ã£o tambÃ©m de valores presentes na coluna `Produto`.

## 6. Tratamento da VariÃ¡vel `Marca`

- CorreÃ§Ãµes e padronizaÃ§Ãµes nos valores da coluna `Marca`.
- ExclusÃ£o de termos genÃ©ricos ou invÃ¡lidos substituindo-os por `'Nao Informado'`.

## 7. SubstituiÃ§Ã£o da Coluna `URL` por `ID`

- Leitura do dataset `IDs_URLs.csv`.
- JunÃ§Ã£o com o dataset tratado com base na coluna `URL`.
- RemoÃ§Ã£o da coluna `URL`.

## 8. ExportaÃ§Ã£o do Dataset

- Escrita final dos dados tratados no formato `.parquet` com o nome `df1_tratado.parquet`.

---

## ConsideraÃ§Ãµes Finais

O dataset `df1` passou por um processo completo de ETL (Extract, Transform, Load), garantindo:

- Dados limpos e estruturados;
- Tipos de dados coerentes para anÃ¡lise;
- CategorizaÃ§Ã£o uniforme e padronizada;
- Base pronta para integraÃ§Ã£o e uso em pipelines de anÃ¡lise de dados ou machine learning.

# DocumentaÃ§Ã£o de Tratamento de Dados â€” Dataset `df2`

## ğŸ“Œ Objetivo
Este notebook realiza o tratamento do dataset `df2.csv`, contendo informaÃ§Ãµes de comentÃ¡rios de produtos. O tratamento tem como finalidade a limpeza dos dados e a substituiÃ§Ã£o da variÃ¡vel `URL` pela variÃ¡vel `ID`, utilizando uma tabela auxiliar.

---

## âš™ï¸ Etapas do Processamento

### 1. CriaÃ§Ã£o da SparkSession
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Prata3").getOrCreate()
```
InicializaÃ§Ã£o do ambiente Spark para manipulaÃ§Ã£o dos dados.

---

### 2. ImportaÃ§Ãµes
```python
from pyspark.sql.functions import col, sum, count, when, countDistinct
```
FunÃ§Ãµes auxiliares para operaÃ§Ãµes de filtragem, contagem e renomeaÃ§Ã£o de colunas.

---

### 3. Leitura dos Dados
```python
df = spark.read.csv("dados\df2.csv", sep=";", header=True)
```
Leitura do dataset `df2.csv` com separador `;` e cabeÃ§alho.

---

### 4. VisualizaÃ§Ã£o e Contagem Inicial
```python
df.show()
df.count()
```
VisualizaÃ§Ã£o inicial dos dados e contagem do nÃºmero total de registros.

---

### 5. RenomeaÃ§Ã£o de Coluna
```python
df = df.withColumnRenamed("ComentÃ¡rios", "Comentarios")
```
PadronizaÃ§Ã£o do nome da coluna `ComentÃ¡rios` para `Comentarios`.

---

### 6. VerificaÃ§Ã£o de Valores Nulos
```python
df.filter(col("Comentarios").isNull()).count()
df.select(count(when(df["Comentarios"].isNull(), True)).alias("Quant_Nulos")).show()
```
Contagem de registros com valores nulos na coluna `Comentarios`.

---

### 7. RemoÃ§Ã£o de Registros com Nulos
```python
df_sem_nulos = df.dropna()
df_sem_nulos.count()
df_sem_nulos.select(count(when(col("Comentarios").isNull(),True)).alias("Quant_Nulos")).show()
```
RemoÃ§Ã£o de registros com valores nulos e verificaÃ§Ã£o pÃ³s-tratamento.

---

### 8. VerificaÃ§Ã£o de Unicidade da Coluna `URL`
```python
df.select(countDistinct(col("URL"))).show()
df_sem_nulos.select(countDistinct(col("URL"))).show()
```
VerificaÃ§Ã£o da quantidade de URLs distintas antes e depois do tratamento.

---

### 9. SubstituiÃ§Ã£o da Coluna `URL` por `ID`
```python
id_urls = spark.read.csv("dados\IDs_URLs.csv", sep=";", header=True)
df2_tratado = id_urls.join(df_sem_nulos, on="URL", how="inner").drop("URL")
```
UtilizaÃ§Ã£o do dataset `IDs_URLs.csv` para substituir a coluna `URL` pela `ID` correspondente.

---

### 10. Escrita do Dataset Final
```python
df2_tratado.write.parquet("dados\df2_tratado.parquet")
```
Salvamento do dataset tratado em formato `.parquet`.

---

### 11. VerificaÃ§Ã£o Final do Schema
```python
df2_tratado.printSchema()
```
VisualizaÃ§Ã£o do schema final para conferÃªncia dos tipos e nomes das colunas.

---

## âœ… Resultado
O dataset `df2` foi tratado com sucesso, resultando em um arquivo `df2_tratado.parquet` sem valores nulos e com a substituiÃ§Ã£o de `URL` por `ID`.


# DocumentaÃ§Ã£o de Tratamento de Dados â€” Dataset `df3`

## ğŸ¯ Objetivo

Realizar o tratamento dos dados contidos no arquivo `df3.csv`, padronizando valores, removendo inconsistÃªncias e preparando o dataset para integraÃ§Ãµes e anÃ¡lises posteriores.

---

## ğŸ”§ ImportaÃ§Ãµes

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import count_distinct, col, when, sum, regexp_extract
```

---

## ğŸš€ InicializaÃ§Ã£o da SparkSession

```python
spark = SparkSession.builder.appName('Prata2').getOrCreate()
```

CriaÃ§Ã£o da sessÃ£o Spark com o nome `Prata2`.

---

## ğŸ“¥ Leitura dos Dados

```python
df = spark.read.csv("dados\\df3.csv", sep=";", header=True)
df.show()
df.count()
df.printSchema()
```

Leitura do arquivo CSV contendo as avaliaÃ§Ãµes de caracterÃ­sticas dos produtos. Exibe os dados, conta as linhas e imprime o schema.

---

## âœï¸ RenomeaÃ§Ã£o de Colunas

```python
df = df.withColumnRenamed("Aval.CaracterÃ­stica", "Aval_Caracteristica") \       .withColumnRenamed("CaracterÃ­stica", "Caracteristica")
```

PadronizaÃ§Ã£o dos nomes de colunas para evitar problemas com caracteres especiais.

---

## ğŸ” AnÃ¡lise de Distintos

```python
df.select("Caracteristica").distinct().withColumnRenamed("Caracteristica", "Caracteristiscas_Unicas").show(truncate=False)
df.select(count_distinct(col("Caracteristica")).alias("Caracteristicas_Distintas")).show()
```

Verifica a diversidade de caracterÃ­sticas no dataset.

---

## ğŸ” PadronizaÃ§Ã£o de Valores CategÃ³ricos

```python
df1 = df.withColumn("Caracteristica", when(col("Caracteristica") == "RelaciÃ³n precio-calidad", "RelaÃ§Ã£o preÃ§o-qualidade")
                    .otherwise(col("Caracteristica"))) \        .withColumn("Caracteristica", when(col("Caracteristica") == "Calidad de los materiais", "Qualidade dos materiais")
                    .otherwise(col("Caracteristica")))
```

Padroniza valores escritos em espanhol para o portuguÃªs.

---

## â“ VerificaÃ§Ã£o de Valores Nulos

```python
nulos = df.select([
    sum(col("Aval_Caracteristica").isNull().cast("int")).alias("Quant_Nulos"),
    sum(col("Caracteristica").isNull().cast("int")).alias("Quant_Caracteristica"),
    sum(col("URL").isNull().cast("int")).alias("Quant_URL")
])
nulos.show()
```

Conta os valores nulos em cada coluna.

---

## ğŸ”¢ ExtraÃ§Ã£o de NÃºmeros da Coluna de AvaliaÃ§Ã£o

```python
df2 = df.withColumn("Aval_Caracteristica", regexp_extract("Aval_Caracteristica", r"(\d+(\.\d+)?)", 1))
df2.show()
```

Extrai os valores numÃ©ricos das strings na coluna `Aval_Caracteristica`.

---

## ğŸ”„ ConversÃ£o de Tipos

```python
df3 = df2.withColumn("Aval_Caracteristica", col("Aval_Caracteristica").cast("double"))
df3.printSchema()
df3.show()
df3.count()
```

Converte a coluna de avaliaÃ§Ã£o para tipo `double`.

---

## ğŸ”— JunÃ§Ã£o com IDs de URLs

```python
id_urls = spark.read.csv("dados\\IDs_URLs.csv", sep=";", header=True)
df3_tratado = id_urls.join(df3, on="URL", how="inner").drop("URL")
df3_tratado.show()
```

Relaciona os dados tratados com um identificador Ãºnico por URL, removendo a URL do dataset final.

---

## ğŸ’¾ Escrita do Dataset Tratado

```python
df3_tratado.write.parquet("dados\\df3_tratado.parquet")
df3_tratado.printSchema()
```

Escreve os dados tratados no formato Parquet para uso futuro.

---

## âœ… ConsideraÃ§Ãµes Finais

O dataset `df3` foi padronizado, limpo e enriquecido com identificadores Ãºnicos. Isso permite sua integraÃ§Ã£o com outras tabelas e anÃ¡lise em modelos como o modelo estrela.

# ğŸ“„ DocumentaÃ§Ã£o ConstruÃ§Ã£o do Modelo Estrela

## ğŸ¯ Objetivo

Este notebook realiza a transformaÃ§Ã£o e modelagem dos dados coletados via web scraping, estruturando-os em um **modelo estrela** para anÃ¡lises de BI. As etapas envolvem o cÃ¡lculo de mÃ©tricas derivadas, categorizaÃ§Ã£o de preÃ§os, identificaÃ§Ã£o de produtos destaque e a construÃ§Ã£o de tabelas fato e dimensÃµes.

---

## ğŸ“¦ ImportaÃ§Ãµes

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum, when, countDistinct, min, max, avg, round, lit, row_number, desc, asc, log1p, percent_rank, upper
from pyspark.sql.window import Window
```

FunÃ§Ãµes utilizadas para agregaÃ§Ãµes, transformaÃ§Ãµes e janelas de anÃ¡lise.

---

## ğŸš€ InicializaÃ§Ã£o da SparkSession

```python
spark = SparkSession.builder.appName('tabelas').getOrCreate()
```

Cria o ambiente Spark necessÃ¡rio para manipulaÃ§Ã£o dos dados.

---

## ğŸ“¥ Carregamento dos Datasets Tratados

```python
df1 = spark.read.parquet(".../df1_tratado.parquet")
df3 = spark.read.parquet(".../df3_tratado.parquet")
df4 = spark.read.csv(".../IDs_URLs.csv", sep=';', header=True)
```

Leitura dos datasets finais jÃ¡ tratados e prontos para modelagem.

---

## ğŸ“Š AvaliaÃ§Ã£o Ponderada

```python
media_avaliacoes_produtos = df1.select(avg(col("Avaliacao"))).collect()[0][0]
m = 10  # valor mÃ­nimo de confianÃ§a
df_transicao = df1.withColumn(
    'avaliacao_ponderada',
    (col('Quant_Avaliacoes') / (col('Quant_Avaliacoes') + lit(m))) * col('Avaliacao') +
    ((lit(m) / (col('Quant_Avaliacoes') + lit(m))) * lit(media_avaliacoes_produtos)))
```

Calcula uma avaliaÃ§Ã£o mais justa ponderando pela quantidade de avaliaÃ§Ãµes.

---

## ğŸ“ˆ Escore de Engajamento

```python
df_transicao = df_transicao.withColumn(
    'Escore_Engajamento',
    log1p(col('Quant_Avaliacoes') + col('Quant_Comentarios')))
```

Cria um escore baseado na soma de avaliaÃ§Ãµes e comentÃ¡rios com escala logarÃ­tmica.

---

## ğŸ’° Escore Custo-BenefÃ­cio

```python
df_transicao = df_transicao.withColumn(
    'Escore_Custo_Beneficio',
    when(col('Preco') > 0, col('avaliacao_ponderada') / col('Preco')).otherwise(lit(None)))
```

Mede a relaÃ§Ã£o custo-benefÃ­cio do produto.

---

## ğŸ·ï¸ Faixa de PreÃ§o

```python
quantis = df_transicao.approxQuantile("Preco", [0.33, 0.66], 0.01)
df_transicao = df_transicao.withColumn(
    "Faixa_Preco",
    when(col("Preco") <= quantis[0], "Baixo")
    .when((col("Preco") > quantis[0]) & (col("Preco") <= quantis[1]), "MÃ©dio")
    .otherwise("Alto"))
```

Agrupamento de produtos por faixa de preÃ§o baseada em quantis.

---

## ğŸŒŸ Produto Destaque

```python
mediana_engajamento = df_transicao.approxQuantile("Escore_Engajamento", [0.5], 0.01)[0]
df_transicao = df_transicao.withColumn(
    "Produto_Destaque",
    when((col("avaliacao_ponderada") >= 4.5) & (col("Escore_Engajamento") >= mediana_engajamento), True).otherwise(False))
```

Marca produtos com boa avaliaÃ§Ã£o e alto engajamento como destaque.

---

## ğŸ CriaÃ§Ã£o da Tabela Fato

```python
fato_avaliacoes_produto = df_transicao
```

Tabela principal que consolida todas as mÃ©tricas calculadas.

---

## ğŸ“¦ Tabelas de DimensÃ£o

### Produto

```python
dim_produto = df1.select('ID', 'Produto', 'Categoria', 'Marca')
```

### CaracterÃ­sticas

```python
dim_caracteristicas = df3
```

### URLs

```python
dim_url = df4
```

### Marca

```python
dim_marca = fato_avaliacoes_produto.groupBy("Marca").agg(...).withColumn("Faixa_Preco_Marca", ...)
```

### Categoria

```python
dim_categoria = fato_avaliacoes_produto.groupBy("Categoria").agg(...).withColumn("Faixa_Preco_Categoria", ...)
```

Cada dimensÃ£o agrega dados por chave descritiva, com mÃ©tricas e classificaÃ§Ãµes por faixa.

---

## ğŸ’¾ Escrita dos Dados

```python
fato_avaliacoes_produto.write.parquet(".../fato_avaliacoes_produto")
dim_produto.write.parquet(".../dim_produto")
dim_caracteristicas.write.parquet(".../dim_caracteristicas")
dim_url.write.parquet(".../dim_url")
dim_marca.write.parquet(".../dim_marca")
dim_categoria.write.parquet(".../dim_categoria")
```

Armazena todas as tabelas geradas no formato `.parquet`.

---

## âœ… Resultado Final

Estrutura final em modelo estrela, pronta para anÃ¡lise em ferramentas de BI ou exploraÃ§Ã£o com SparkSQL.
ğŸ“„ DocumentaÃ§Ã£o AnÃ¡lise de Sentimentos com BERT e SpaCy
ğŸ¯ Objetivo
Este notebook realiza a anÃ¡lise de sentimentos de comentÃ¡rios em portuguÃªs utilizando o modelo BERT multilÃ­ngue e o modelo linguÃ­stico do SpaCy para extraÃ§Ã£o de adjetivos. O objetivo Ã© classificar os sentimentos dos comentÃ¡rios, identificar palavras-chave e gerar um dataset enriquecido para anÃ¡lises qualitativas.

ğŸ“¦ ImportaÃ§Ãµes e Upload dos Dados

Realiza o upload e concatenaÃ§Ã£o de mÃºltiplos arquivos .parquet contendo comentÃ¡rios.

ğŸ§± Estrutura Inicial dos Dados

VisualizaÃ§Ã£o e verificaÃ§Ã£o da estrutura dos dados carregados.

ğŸ› ï¸ InstalaÃ§Ã£o de DependÃªncias

Instala os pacotes necessÃ¡rios para anÃ¡lise de sentimentos e processamento de linguagem natural.

ğŸ“š Carregamento de Modelos

Inicializa os modelos de NLP para anÃ¡lise de sentimentos e extraÃ§Ã£o de adjetivos.

ğŸ§  FunÃ§Ãµes de Processamento
AnÃ¡lise de Sentimento

Classifica o sentimento do comentÃ¡rio com base na escala de estrelas do modelo BERT.

ExtraÃ§Ã£o de Adjetivo Principal

Extrai o primeiro adjetivo do comentÃ¡rio como palavra-chave representativa.

ClassificaÃ§Ã£o do Adjetivo

Classifica o sentimento da palavra-chave extraÃ­da.

ğŸ§ª AplicaÃ§Ã£o das FunÃ§Ãµes

Aplica as funÃ§Ãµes de anÃ¡lise de sentimentos e extraÃ§Ã£o de palavras-chave ao DataFrame.

ğŸ’¾ Escrita e Download do Resultado

Salva o resultado final em formato .parquet e disponibiliza para download.

âœ… Resultado Final
Dataset enriquecido com colunas de sentimento geral, palavra-chave extraÃ­da e percepÃ§Ã£o da palavra, pronto para anÃ¡lises qualitativas, dashboards ou integraÃ§Ã£o com pipelines de NLP.
