{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "087d585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site para extração\n",
    "url = \"https://lista.mercadolivre.com.br/_Container_pet-cpg-caes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee1eae0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\fred\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\fred\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\fred\\anaconda3\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: idna in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\fred\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\fred\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "# Instalação de bibliotecas\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df06e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59ed4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para usar o Selenium\n",
    "\n",
    "edge_service = Service(r\"C:\\Users\\fred\\Documents\\CursoFullStack\\Web Scraping\\edgedriver_win64\\msedgedriver.exe\")\n",
    "options = webdriver.EdgeOptions()\n",
    "driver = webdriver.Edge(service= edge_service, options=options)\n",
    "\n",
    "# Iniciando o Selenium\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Obtendo as urls de cada produto\n",
    "\n",
    "produtos_links = []\n",
    "\n",
    "\n",
    "title_wrappers = driver.find_elements(By.CLASS_NAME,\"poly-component__title-wrapper\")\n",
    "for wrapper in title_wrappers:\n",
    "    try:\n",
    "        link_element = wrapper.find_element(By.TAG_NAME, \"a\")\n",
    "        href = link_element.get_attribute(\"href\")\n",
    "        if href and \"mercadolivre.com.br\" in href:\n",
    "            produtos_links.append(href)\n",
    "    except:\n",
    "        continue\n",
    "next_button = driver.find_element(By.XPATH, \"//li[contains(@class, 'andes-pagination__button--next')]/a\")\n",
    "next_page_url = next_button.get_attribute(\"href\")\n",
    "driver.get(next_page_url)  \n",
    "time.sleep(10)\n",
    "\n",
    "while True:\n",
    "    title_wrappers = driver.find_elements(By.CLASS_NAME,\"ui-search-item__title\")\n",
    "    for wrapper in title_wrappers:\n",
    "        try:\n",
    "            link_element = wrapper.find_element(By.TAG_NAME, \"a\")\n",
    "            href = link_element.get_attribute(\"href\")\n",
    "            if href and \"mercadolivre.com.br\" in href:\n",
    "                produtos_links.append(href)\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//li[contains(@class, 'andes-pagination__button--next')]/a\")\n",
    "        next_page_url = next_button.get_attribute(\"href\")\n",
    "        driver.get(next_page_url)  \n",
    "        time.sleep(10)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(produtos_links)\n",
    "df.to_csv('URLs_por_Produto.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70125099",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Recebendo dataset com Urls únicos\n",
    "\n",
    "df_urls = pd.read_csv('URLs_por_produto.csv')\n",
    "product_links = df_urls.iloc[:, 0].tolist() \n",
    "\n",
    "# Tabela 1 - Informações gerais\n",
    "\n",
    "dataset1 = {'Categoria': [],\n",
    "          'Produto': [],\n",
    "          'Marca': [], \n",
    "          'Preco': [],\n",
    "          'Avaliacao': [],\n",
    "          'Quant. Avaliacoes': [],\n",
    "          'Quant. Comentários': [],\n",
    "          'URL': []}\n",
    "\n",
    "# Tabela 2 - Comentários\n",
    "\n",
    "dataset2 = {'URL': [],\n",
    "           'Comentários': []}\n",
    "\n",
    "\n",
    "# Tabela 3 - Avaliação de características\n",
    "\n",
    "dataset3 = {'Característica': [],\n",
    "           'Aval.Característica': [],\n",
    "           'URL': []}\n",
    "\n",
    "# Tabela 4 - Urls que não responderam\n",
    "\n",
    "nao_capturado = []\n",
    "\n",
    "# Declaração de variável para intervalo de requisições\n",
    "\n",
    "requisicoes = 0\n",
    "comentario = []\n",
    "caracteristica = []\n",
    "avaliacao_caracteristica = []\n",
    "\n",
    "# Código a ser aplicado em cada url única\n",
    "\n",
    "for pag in product_links:\n",
    "    try:\n",
    "        req_books = requests.get(pag)\n",
    "        conteudo_books = BeautifulSoup(req_books.text,'html.parser')\n",
    "\n",
    "        # Variável para nome do produto\n",
    "\n",
    "        produto = conteudo_books.find('h1', class_=\"ui-pdp-title\").text\n",
    "\n",
    "        # Confirmação da declaração da variável produto\n",
    "\n",
    "        max_tentativas = 10\n",
    "        tentativa = 0\n",
    "        while tentativa < max_tentativas and produto is None:\n",
    "            req_books = requests.get(pag)\n",
    "            conteudo_books = BeautifulSoup(req_books.text,'html.parser')\n",
    "            tentativa += 1\n",
    "            produto = conteudo_books.find('h1', class_=\"ui-pdp-title\").text\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Variáveis Gerais\n",
    "\n",
    "        preco_tag = conteudo_books.find(\"meta\", itemprop=\"price\")\n",
    "        preco = preco_tag.get(\"content\") if preco_tag else None\n",
    "        cate_tag = conteudo_books.find_all(\"a\", class_=\"andes-breadcrumb__link\")\n",
    "        categoria = cate_tag[-1].get(\"title\")\n",
    "        avaliacao_tag = conteudo_books.find('p', class_=\"ui-review-capability__rating__average ui-review-capability__rating__average--desktop\")\n",
    "        avaliacao = avaliacao_tag.text if avaliacao_tag else None\n",
    "        quant_avalia_tag = conteudo_books.find(\"span\", class_=\"ui-pdp-review__amount\")\n",
    "        quant_avaliacao = quant_avalia_tag.text.strip('()') if avaliacao_tag else None\n",
    "        quant_comentario_tag = conteudo_books.find('span', class_=\"total-opinion\")\n",
    "        quant_comentario = conteudo_books.find('span', class_=\"total-opinion\").text if quant_comentario_tag else None\n",
    "\n",
    "        # Confirmação da declaração da variável marca\n",
    "\n",
    "        max_tentativas = 10\n",
    "        tentativa = 0\n",
    "        marca = None\n",
    "        while tentativa < max_tentativas and marca is None:\n",
    "            req_books = requests.get(pag)\n",
    "            conteudo_books = BeautifulSoup(req_books.text,'html.parser')\n",
    "            tentativa += 1\n",
    "            marca = None  \n",
    "            tabelas = []\n",
    "            tabelas = conteudo_books.find_all(\"table\", class_=\"andes-table\")\n",
    "\n",
    "            for tabela in tabelas:\n",
    "                for linha in tabela.find_all(\"tr\"):\n",
    "                    th_div = linha.find(\"div\", class_=\"andes-table__header__container\")\n",
    "                    if th_div and \"Marca\" in th_div.get_text(strip=True): \n",
    "                        td = linha.find(\"td\")  \n",
    "                        if td:\n",
    "                            marca = td.get_text(strip=True) \n",
    "                            break  \n",
    "                if marca:\n",
    "                    break  \n",
    "\n",
    "        # Variáveis para a tabela 1\n",
    "\n",
    "        dataset1['Categoria'].append(categoria)\n",
    "        dataset1['Produto'].append(produto)\n",
    "        dataset1['Preco'].append(preco)\n",
    "        dataset1['Avaliacao'].append(avaliacao)\n",
    "        dataset1['Quant. Avaliacoes'].append(quant_avaliacao)\n",
    "        dataset1['Marca'].append(marca)\n",
    "        dataset1['URL'].append(pag)\n",
    "        dataset1['Quant. Comentários'].append(quant_comentario)\n",
    "\n",
    "        # Variáveis para avaliação de características\n",
    "\n",
    "        aval_produto = conteudo_books.findAll('tr', class_=\"ui-review-capability-categories__desktop--row\")\n",
    "        if aval_produto is not None:\n",
    "            for a in aval_produto:\n",
    "\n",
    "                # Variáveis para a tabela 3\n",
    "\n",
    "                dataset3['Característica'].append(a.find('td', class_=\"\").text)\n",
    "                dataset3['Aval.Característica'].append(a.find('p', class_=\"andes-visually-hidden\").text)\n",
    "                dataset3['URL'].append(pag)   \n",
    "\n",
    "        # Declaração da variável comentário\n",
    "\n",
    "        # Verificação se há comentários\n",
    "\n",
    "        if avaliacao is not None:\n",
    "            botao = conteudo_books.find(\"button\", {\"data-testid\": \"see-more\"})\n",
    "\n",
    "            # Se houver comentários\n",
    "\n",
    "            # Verificação se há necessidade de usar Selenium\n",
    "\n",
    "            if botao:\n",
    "\n",
    "                # Usando o Selenium\n",
    "\n",
    "                def open_and_scrape(pag):\n",
    "                    options = webdriver.EdgeOptions()\n",
    "                    options.add_argument(\"--start-maximized\")\n",
    "\n",
    "                    driver = webdriver.Edge(options=options)\n",
    "                    driver.get(pag)\n",
    "\n",
    "                    try:\n",
    "                        time.sleep(3)\n",
    "\n",
    "                        # Simular rolagem para carregar mais comentários\n",
    "                        body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                        for _ in range(15): \n",
    "                            body.send_keys(Keys.PAGE_DOWN)\n",
    "                            time.sleep(1)\n",
    "\n",
    "                        # Clicar no botão \"Ver mais\"\n",
    "                        try:\n",
    "                            see_more_button = WebDriverWait(driver, 10).until(\n",
    "                                EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[data-testid='see-more']\"))\n",
    "                            )\n",
    "                            see_more_button.click()\n",
    "                            time.sleep(3) \n",
    "                        except:\n",
    "                            print(\"Botão 'Ver mais' não encontrado, continuando...\")\n",
    "\n",
    "                        # Declaração da variável com url do modal\n",
    "\n",
    "                        iframe = WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.ID, \"ui-pdp-iframe-reviews\"))\n",
    "                        )\n",
    "                        iframe_url = iframe.get_attribute(\"src\")\n",
    "\n",
    "                        # Garantir que a URL do modal seja completa\n",
    "                        if \"http\" not in iframe_url:\n",
    "                            iframe_url = \"https://www.mercadolivre.com.br\" + iframe_url\n",
    "\n",
    "\n",
    "                        # Abrir a URL do modal diretamente\n",
    "\n",
    "                        driver.get(iframe_url)\n",
    "                        time.sleep(5)\n",
    "\n",
    "                        # Pressionar TAB algumas vezes para garantir que o foco está no modal\n",
    "\n",
    "                        actions = ActionChains(driver)\n",
    "                        actions.send_keys(Keys.TAB).perform()\n",
    "                        time.sleep(1)\n",
    "\n",
    "\n",
    "                        # Simula o pressionamento de Page Down\n",
    "\n",
    "                        prev_scroll_top = -1  # Inicializa uma variável para armazenar a posição anterior\n",
    "\n",
    "                        while True:\n",
    "                            # Executa o comando para rolar para baixo\n",
    "                            actions.send_keys(Keys.PAGE_DOWN).perform()\n",
    "                            time.sleep(1)  # Aguarda um pouco para a página carregar\n",
    "\n",
    "                            # Executa JavaScript para obter a posição do scroll\n",
    "                            scroll_top = driver.execute_script(\"return window.pageYOffset;\")\n",
    "                            scroll_height = driver.execute_script(\"return document.documentElement.scrollHeight;\")\n",
    "                            client_height = driver.execute_script(\"return window.innerHeight;\")\n",
    "\n",
    "                            # Se a posição do scroll não mudou ou atingiu o final da página, sair do loop\n",
    "                            if scroll_top == prev_scroll_top or (scroll_top + client_height) >= scroll_height:\n",
    "                                break\n",
    "\n",
    "                            prev_scroll_top = scroll_top\n",
    "\n",
    "                        # Capturar o HTML atualizado\n",
    "\n",
    "                        page_source = driver.page_source\n",
    "\n",
    "\n",
    "                    finally:\n",
    "                        driver.quit()\n",
    "\n",
    "                    # Usar BeautifulSoup para extrair os comentários\n",
    "                    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "\n",
    "                    # Encontrar todas as tags <p> dos comentários\n",
    "                    for comment in soup.find_all(\"p\", class_=\"ui-review-capability-comments__comment__content\"):\n",
    "                        dataset2['Comentários'].append(comment.get_text(strip=True))\n",
    "                        dataset2['URL'].append(pag)\n",
    "                # Executar função\n",
    "\n",
    "                open_and_scrape(pag)\n",
    "\n",
    "            # Se não houver necessidade de usar o Selenium\n",
    "\n",
    "            else:\n",
    "                for comment in conteudo_books.find_all(\"p\", class_=\"ui-review-capability-comments__comment__content\"):\n",
    "                    dataset2['Comentários'].append(comment.get_text(strip=True))\n",
    "                    dataset2['URL'].append(pag)\n",
    "        # Se não tem comentários\n",
    "\n",
    "        else:\n",
    "            dataset2['Comentários'].append('Sem comentários')\n",
    "            dataset2['URL'].append(pag)\n",
    "\n",
    "\n",
    "\n",
    "        # Limitador de requisições em sequência para evitar bloqueio\n",
    "\n",
    "        requisicoes +=1\n",
    "        print(requisicoes)\n",
    "        if requisicoes == 100:\n",
    "            print('100 requisições. Pausa de 30 minutos')\n",
    "            requisicoes = 0\n",
    "            time.sleep(1805)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    except Exception as e:\n",
    "        nao_capturado.append(pag)\n",
    "        continue  # Vai para o próximo `i` sem interromper o loop\n",
    "\n",
    "# Transformando em DataFrame e exportando para csv\n",
    "\n",
    "df1 = pd.DataFrame(dataset1)\n",
    "df2 = pd.DataFrame(dataset2)\n",
    "df3 = pd.DataFrame(dataset3)\n",
    "urls_faltantes = pd.DataFrame(nao_capturado)\n",
    "df1.to_csv('df1.csv', sep = ';', index=False, encoding=\"utf-8-sig\")\n",
    "df2.to_csv('df2.csv', sep = ';', index=False, encoding=\"utf-8-sig\")\n",
    "df3.to_csv('df3.csv', sep = ';', index=False, encoding=\"utf-8-sig\")\n",
    "urls_faltantes.to_csv('urls_faltantes.csv',sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
