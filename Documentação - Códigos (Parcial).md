
# üßæ Documenta√ß√£o T√©cnica - Web Scraping Mercado Livre

Este script realiza a **extra√ß√£o automatizada de dados de produtos para c√£es** no site Mercado Livre, utilizando `Selenium`, `BeautifulSoup` e `Requests`.

---

## üì¶ Requisitos

Instale os pacotes necess√°rios:

```bash
pip install requests
pip install beautifulsoup4
pip install selenium
```

---

## üîó URL de origem

```python
url = "https://lista.mercadolivre.com.br/_Container_pet-cpg-caes"
```

---

## üöÄ 1. Inicializa√ß√£o do Selenium

```python
from selenium import webdriver
from selenium.webdriver.edge.service import Service

edge_service = Service(r"Caminho\para\msedgedriver.exe")
options = webdriver.EdgeOptions()
driver = webdriver.Edge(service=edge_service, options=options)
driver.get(url)
```

- Inicia o navegador Edge automatizado
- Acessa a URL de listagem de produtos

---

## üîÑ 2. Extra√ß√£o de links de produtos

```python
produtos_links = []

title_wrappers = driver.find_elements(By.CLASS_NAME,"poly-component__title-wrapper")
for wrapper in title_wrappers:
    link_element = wrapper.find_element(By.TAG_NAME, "a")
    produtos_links.append(link_element.get_attribute("href"))
```

- Captura os links dos produtos em cada p√°gina
- Navega para a pr√≥xima p√°gina via bot√£o "Pr√≥ximo"
- Repeti√ß√£o at√© n√£o haver mais p√°ginas

---

## üíæ 3. Armazenamento dos links

```python
df = pd.DataFrame(produtos_links)
df.to_csv('URLs_por_Produto.csv', index=False)
```

- Links s√£o salvos em `URLs_por_Produto.csv`

---

## üßÆ 4. Estrutura dos Datasets

S√£o utilizados **4 dicion√°rios principais**:

```python
dataset1 = {'Categoria': [], 'Produto': [], 'Marca': [], 'Preco': [], 'Avaliacao': [], 'Quant. Avaliacoes': [], 'Quant. Coment√°rios': [], 'URL': []}
dataset2 = {'URL': [], 'Coment√°rios': []}
dataset3 = {'Caracter√≠stica': [], 'Aval.Caracter√≠stica': [], 'URL': []}
nao_capturado = []
```

---

## üîç 5. Loop de extra√ß√£o por URL

```python
for pag in product_links:
    try:
        req_books = requests.get(pag)
        conteudo_books = BeautifulSoup(req_books.text, 'html.parser')
        ...
    except Exception as e:
        nao_capturado.append(pag)
```

Cada URL passa pelas seguintes etapas:

### üõí Informa√ß√µes gerais

- `produto`: t√≠tulo da p√°gina
- `preco`: valor extra√≠do de `<meta itemprop="price">`
- `categoria`: breadcrumb final
- `avaliacao`: m√©dia de notas
- `quant_avaliacao`: quantidade de avalia√ß√µes
- `quant_comentario`: quantidade de coment√°rios (se existir)
- `marca`: varre as tabelas para encontrar linha com t√≠tulo "Marca"

### üó£Ô∏è Coment√°rios

Dois fluxos poss√≠veis:

1. **Com modal (iframe)**: usa Selenium para abrir coment√°rios em p√°gina nova  
2. **Sem modal**: extrai diretamente com BeautifulSoup

### üìä Caracter√≠sticas avaliadas

- Encontra tabelas com classe `ui-review-capability-categories__desktop--row`
- Extrai: `Caracter√≠stica`, `Avalia√ß√£o`, e URL

---

## üìâ 6. Controle de Requisi√ß√µes

```python
requisicoes += 1
if requisicoes == 100:
    time.sleep(1805)
```

Evita bloqueio do servidor fazendo pausa de 30 minutos a cada 100 requisi√ß√µes.

---

## üì§ 7. Exporta√ß√£o dos resultados

```python
pd.DataFrame(dataset1).to_csv('df1.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(dataset2).to_csv('df2.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(dataset3).to_csv('df3.csv', sep=';', encoding='utf-8-sig', index=False)
pd.DataFrame(nao_capturado).to_csv('urls_faltantes.csv', sep=';')
```

---

## üß† Observa√ß√µes T√©cnicas

- A captura de algumas informa√ß√µes podem falhar devido a falha ou demora de carregamento do html. Isso √© tratado com tentativas repetidas.
- A rolagem infinita √© simulada via `PAGE_DOWN` e checagem de `scrollTop`.
- Bot√£o ‚ÄúVer mais‚Äù de coment√°rios √© clicado com `Selenium` e `ActionChains`.

---

## ‚úÖ Sugest√µes de Refatora√ß√£o

- Modularizar cada etapa (coleta de links, coleta de dados, coleta de coment√°rios)
- Centralizar controle de exce√ß√µes e log
- Criar arquivos `.py` separados para scraping e processamento
- Parametrizar caminho do driver e URL de origem
- Adicionar `try/except` com tipos de exce√ß√£o espec√≠ficas
- Otimizar checagem de ‚ÄúMarca‚Äù com regex

---

## üß™ Testes e Valida√ß√£o

- Testado com dezenas de produtos da categoria PET
- URLs com falhas s√£o salvas em `urls_faltantes.csv`
- C√≥digo resiste a pequenas varia√ß√µes de layout

---

# Documenta√ß√£o do Pr√©-processamento do Dataset `df1`

Este documento descreve o tratamento aplicado ao dataset `df1`, desenvolvido em Apache Spark com PySpark. O objetivo foi limpar, padronizar e preparar os dados para an√°lises posteriores.

## Sum√°rio

- [1. Carregamento dos Dados](#1-carregamento-dos-dados)
- [2. Padroniza√ß√£o de Vari√°veis e Valores](#2-padroniza√ß√£o-de-vari√°veis-e-valores)
- [3. Tratamento de Campos Nulos](#3-tratamento-de-campos-nulos)
- [4. Remo√ß√£o de Registros Duplicados](#4-remo√ß√£o-de-registros-duplicados)
- [5. Tratamento da Vari√°vel `Categoria`](#5-tratamento-da-vari√°vel-categoria)
- [6. Tratamento da Vari√°vel `Marca`](#6-tratamento-da-vari√°vel-marca)
- [7. Substitui√ß√£o da Coluna `URL` por `ID`](#7-substitui√ß√£o-da-coluna-url-por-id)
- [8. Exporta√ß√£o do Dataset](#8-exporta√ß√£o-do-dataset)

---

## 1. Carregamento dos Dados

- Inicializa√ß√£o da SparkSession.
- Leitura do dataset `df1.csv` com `;` como delimitador e cabe√ßalho ativado.

## 2. Padroniza√ß√£o de Vari√°veis e Valores

- Renomeadas colunas para evitar caracteres especiais.
- Substitu√≠dos caracteres n√£o num√©ricos em `Quant_Comentarios`.
- Tipos de dados convertidos para `FloatType` e `IntegerType` quando aplic√°vel.

## 3. Tratamento de Campos Nulos

- Identifica√ß√£o de valores nulos por coluna.
- Substitui√ß√£o de nulos nas colunas `Quant_Avaliacoes` e `Quant_Comentarios` por 0.
- Substitui√ß√£o de nulos na coluna `Marca` por `'Nao Informado'`.

## 4. Remo√ß√£o de Registros Duplicados

- Elimina√ß√£o de registros duplicados com `dropDuplicates()`.

## 5. Tratamento da Vari√°vel `Categoria`

- Normaliza√ß√£o de valores de `Categoria` com base em palavras-chave presentes nas strings.
- Unifica√ß√£o de categorias similares para agrupamentos mais consistentes.
- Considera√ß√£o tamb√©m de valores presentes na coluna `Produto`.

## 6. Tratamento da Vari√°vel `Marca`

- Corre√ß√µes e padroniza√ß√µes nos valores da coluna `Marca`.
- Exclus√£o de termos gen√©ricos ou inv√°lidos substituindo-os por `'Nao Informado'`.

## 7. Substitui√ß√£o da Coluna `URL` por `ID`

- Leitura do dataset `IDs_URLs.csv`.
- Jun√ß√£o com o dataset tratado com base na coluna `URL`.
- Remo√ß√£o da coluna `URL`.

## 8. Exporta√ß√£o do Dataset

- Escrita final dos dados tratados no formato `.parquet` com o nome `df1_tratado.parquet`.

---

## Considera√ß√µes Finais

O dataset `df1` passou por um processo completo de ETL (Extract, Transform, Load), garantindo:

- Dados limpos e estruturados;
- Tipos de dados coerentes para an√°lise;
- Categoriza√ß√£o uniforme e padronizada;
- Base pronta para integra√ß√£o e uso em pipelines de an√°lise de dados ou machine learning.

# Documenta√ß√£o de Tratamento de Dados ‚Äî Dataset `df2`

## üìå Objetivo
Este notebook realiza o tratamento do dataset `df2.csv`, contendo informa√ß√µes de coment√°rios de produtos. O tratamento tem como finalidade a limpeza dos dados e a substitui√ß√£o da vari√°vel `URL` pela vari√°vel `ID`, utilizando uma tabela auxiliar.

---

## ‚öôÔ∏è Etapas do Processamento

### 1. Cria√ß√£o da SparkSession
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Prata3").getOrCreate()
```
Inicializa√ß√£o do ambiente Spark para manipula√ß√£o dos dados.

---

### 2. Importa√ß√µes
```python
from pyspark.sql.functions import col, sum, count, when, countDistinct
```
Fun√ß√µes auxiliares para opera√ß√µes de filtragem, contagem e renomea√ß√£o de colunas.

---

### 3. Leitura dos Dados
```python
df = spark.read.csv("dados\df2.csv", sep=";", header=True)
```
Leitura do dataset `df2.csv` com separador `;` e cabe√ßalho.

---

### 4. Visualiza√ß√£o e Contagem Inicial
```python
df.show()
df.count()
```
Visualiza√ß√£o inicial dos dados e contagem do n√∫mero total de registros.

---

### 5. Renomea√ß√£o de Coluna
```python
df = df.withColumnRenamed("Coment√°rios", "Comentarios")
```
Padroniza√ß√£o do nome da coluna `Coment√°rios` para `Comentarios`.

---

### 6. Verifica√ß√£o de Valores Nulos
```python
df.filter(col("Comentarios").isNull()).count()
df.select(count(when(df["Comentarios"].isNull(), True)).alias("Quant_Nulos")).show()
```
Contagem de registros com valores nulos na coluna `Comentarios`.

---

### 7. Remo√ß√£o de Registros com Nulos
```python
df_sem_nulos = df.dropna()
df_sem_nulos.count()
df_sem_nulos.select(count(when(col("Comentarios").isNull(),True)).alias("Quant_Nulos")).show()
```
Remo√ß√£o de registros com valores nulos e verifica√ß√£o p√≥s-tratamento.

---

### 8. Verifica√ß√£o de Unicidade da Coluna `URL`
```python
df.select(countDistinct(col("URL"))).show()
df_sem_nulos.select(countDistinct(col("URL"))).show()
```
Verifica√ß√£o da quantidade de URLs distintas antes e depois do tratamento.

---

### 9. Substitui√ß√£o da Coluna `URL` por `ID`
```python
id_urls = spark.read.csv("dados\IDs_URLs.csv", sep=";", header=True)
df2_tratado = id_urls.join(df_sem_nulos, on="URL", how="inner").drop("URL")
```
Utiliza√ß√£o do dataset `IDs_URLs.csv` para substituir a coluna `URL` pela `ID` correspondente.

---

### 10. Escrita do Dataset Final
```python
df2_tratado.write.parquet("dados\df2_tratado.parquet")
```
Salvamento do dataset tratado em formato `.parquet`.

---

### 11. Verifica√ß√£o Final do Schema
```python
df2_tratado.printSchema()
```
Visualiza√ß√£o do schema final para confer√™ncia dos tipos e nomes das colunas.

---

## ‚úÖ Resultado
O dataset `df2` foi tratado com sucesso, resultando em um arquivo `df2_tratado.parquet` sem valores nulos e com a substitui√ß√£o de `URL` por `ID`.


